# Polbert - Polish BERT
Description here...

![PolBERT image](/img/polbert.png)

#Pre-training corpora

Below is the list of corpora used along with the output of 'wc' command (counting lines, words and characters). These corpora were divided into sentences with srxsegmenter (see references), concatenated and tokenized with HuggingFace BERT Tokenizer. 

| Tables        | Lines           | Words  | Characters  |
| ------------- |--------------:| -----:| -----:|
| Polish subset of Open Subtitles      | 236635408| 1431199601 | 7628097730 |
| Polish subset of ParaCrawl     | 8470950      |   176670885 | 1163505275 |
| Polish Parliamentary Corpus | 9799859      |    121154785 | 938896963 |
| Polish Wikipedia | 8014206      |    132067986 | 1015849191 |
| Total | 262920423      |    1861093257 | 10746349159 |

#Pre-training details

#Usage

#Evaluation

#Bias

#Acknowledgements
Tino etc.

#Author

#References
...





model description, training params (dataset, preprocessing, hyperparameters), evaluation results, intended uses & limitations, etc.